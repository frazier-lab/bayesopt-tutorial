{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5: The parallel knowledge gradient acquisition function in BoTorch\n",
    "\n",
    "This tutorial shows two things:\n",
    "- how to use BayesOpt when we can do parallel objective function evaluations\n",
    "- how to use the knowledge-gradient (KG) acquisition function. The KG acquisition function provides better query efficiency than expected improvement (i.e., it typically requires fewer objective function evaluations to find a good solution). KG is also important in certain kinds of grey-box BO, especially multi-information source BO and multi-fidelity BO.\n",
    "\n",
    "This tutorial is based on the BoTorch tutorial, https://botorch.org/tutorials/one_shot_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For problems where we are maximizing and can take batches of size $q$, the *parallel Knowledge Gradient* (KG) acquisition function [1] is defined by:\n",
    "\n",
    "$$\n",
    "\\text{KG}(x_{1:q}) = E_n\\left[\\mu^*_{n+q} - \\mu^*_n \\mid \\text{sample at $x_{1:q}$}\\right]\n",
    "$$\n",
    "where\n",
    "- $\\mu^*_n = \\max_{x} E_n[f(x)]$ is the maximum of the posterior mean after $n$ samples, and \n",
    "- $\\mu^*_{n+q} = \\max_{x} E_{n+q}[f(x)]$ is the maximum of the posterior mean after ]additional samples at the $q$ points $x_{1:q}$.\n",
    "- $E_n$ indicates the expectation with respect to the posterior distribution after $n$ samples.  This expectation is taken over the fantasized samples of $f(x_{1:q})$, which are multivariate normal under this posterior.\n",
    "\n",
    "BoTorch implements a generalization of parallel KG that allows a composition (see [2] for applications of function compositions and computation of the EI acquisition function) with a function $g$ in which: \n",
    "- $\\mu^*_n = \\max_{x} E_n[g(f(x))]$ \n",
    "- $\\mu^*_{n+q} = \\max_{x} E_{n+q}[g(f(x))]$\n",
    "\n",
    "[1] J. Wu and P. Frazier. The parallel knowledge gradient method for batch Bayesian optimization. NIPS 2016.\n",
    "\n",
    "[2] R. Astudillo and P. Frazier. Bayesian optimization of composite functions. ICML 2019.\n",
    "\n",
    "[3] M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: Programmable Bayesian Optimization in PyTorch. ArXiv 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a toy model\n",
    "\n",
    "We'll fit a standard `SingleTaskGP` model on noisy observations of the synthetic function $f(x) = \\sin(2 \\pi x_1) * \\cos(2 \\pi x_2)$ in `d=2` dimensions on the hypercube $[0, 1]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a tensor that will specify our feasible domain,\n",
    "# which is the 2-dimensional unit hypercube\n",
    "bounds = torch.stack([torch.zeros(2), torch.ones(2)])\n",
    "\n",
    "train_X = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(20, 2)\n",
    "train_Y = torch.sin(2 * math.pi * train_X[:, [0]]) * torch.cos(2 * math.pi * train_X[:, [1]])\n",
    "\n",
    "train_Y = standardize(train_Y + 0.05 * torch.randn_like(train_Y))\n",
    "\n",
    "model = SingleTaskGP(train_X, train_Y)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import qKnowledgeGradient\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.sampling import manual_seed\n",
    "\n",
    "# increasing num_fantasies makes the method more accurate, but uses more computation\n",
    "qKG = qKnowledgeGradient(model, num_fantasies=128)\n",
    "\n",
    "# increasing num_restarts and raw_samples makes the method more accurate, but uses more computation\n",
    "with manual_seed(1234):\n",
    "    candidates, acq_value = optimize_acqf(\n",
    "        acq_function=qKG, \n",
    "        bounds=bounds,\n",
    "        q=2,\n",
    "        num_restarts=10,\n",
    "        raw_samples=512,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8250, 0.5266],\n",
       "        [0.9652, 0.5132]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives the set of points that maximize the KG acqusition function.\n",
    "# It contains 2 points because we asked for a batch of size q=2\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1769)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we do not pass a `current_value` argument to optimize_acqf,\n",
    "# acq_value is not actually the KG value, but is instead offset by $\\mu^*_n$.\n",
    "acq_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Use the cells above to write code in which we do batches of q=2 evaluations at a time for minimizing the Hartmann 6 objective function, choosing them using the KG method.  Start with a copy / paste of the code from Exercise 2 in Lab 3.  Create the same output as in that exercise, except each iteration will have 2 points to report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
